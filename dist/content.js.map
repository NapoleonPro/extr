{"version":3,"file":"content.js","mappings":";;;;;;;;;;;;;;AAAA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;ACnFA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;ACvBA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;ACPA;;;;;ACAA;AACA;AACA;AACA;AACA;AACA;AACA;;;;;;;;;;;;ACNA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","sources":["webpack://extr/./src/content/speechRecognition.ts","webpack://extr/webpack/bootstrap","webpack://extr/webpack/runtime/define property getters","webpack://extr/webpack/runtime/hasOwnProperty shorthand","webpack://extr/webpack/runtime/make namespace object","webpack://extr/./src/content/content.ts"],"sourcesContent":["// Speech Recognition Handler\n// File: src/content/speechRecognition.ts\nexport class TranscriptRecognition {\n    constructor(callback) {\n        this.isListening = false;\n        this.transcriptCallback = callback;\n        this.initRecognition();\n    }\n    initRecognition() {\n        // Check if Speech Recognition is supported\n        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n        if (!SpeechRecognition) {\n            console.error('Speech Recognition not supported in this browser');\n            return;\n        }\n        this.recognition = new SpeechRecognition();\n        // Configuration\n        this.recognition.continuous = true; // Keep listening\n        this.recognition.interimResults = true; // Get partial results\n        this.recognition.lang = 'id-ID'; // Bahasa Indonesia (ganti 'en-US' untuk English)\n        this.recognition.maxAlternatives = 1;\n        // Event Handlers\n        this.recognition.onresult = (event) => {\n            this.handleResult(event);\n        };\n        this.recognition.onerror = (event) => {\n            console.error('Speech recognition error:', event.error);\n            // Auto-restart on network error\n            if (event.error === 'network') {\n                console.log('Network error, restarting...');\n                setTimeout(() => this.start(), 1000);\n            }\n        };\n        this.recognition.onend = () => {\n            console.log('Speech recognition ended');\n            // Auto-restart if still supposed to be listening\n            if (this.isListening) {\n                console.log('Auto-restarting recognition...');\n                this.recognition.start();\n            }\n        };\n        console.log('Speech Recognition initialized');\n    }\n    handleResult(event) {\n        // Get the latest result\n        const result = event.results[event.resultIndex];\n        const transcript = result[0].transcript;\n        const confidence = result[0].confidence;\n        const isFinal = result.isFinal;\n        // Send to callback\n        this.transcriptCallback(transcript, isFinal, confidence);\n        // Log for debugging\n        console.log(`[${isFinal ? 'FINAL' : 'INTERIM'}] ${transcript} (confidence: ${confidence?.toFixed(2) || 'N/A'})`);\n    }\n    start() {\n        if (!this.recognition) {\n            console.error('Speech Recognition not initialized');\n            return;\n        }\n        if (this.isListening) {\n            console.log('Already listening');\n            return;\n        }\n        try {\n            this.recognition.start();\n            this.isListening = true;\n            console.log('Started listening...');\n        }\n        catch (error) {\n            console.error('Error starting recognition:', error);\n        }\n    }\n    stop() {\n        if (!this.recognition || !this.isListening) {\n            return;\n        }\n        this.isListening = false;\n        this.recognition.stop();\n        console.log('Stopped listening');\n    }\n    isActive() {\n        return this.isListening;\n    }\n}\n","// The module cache\nvar __webpack_module_cache__ = {};\n\n// The require function\nfunction __webpack_require__(moduleId) {\n\t// Check if module is in cache\n\tvar cachedModule = __webpack_module_cache__[moduleId];\n\tif (cachedModule !== undefined) {\n\t\treturn cachedModule.exports;\n\t}\n\t// Create a new module (and put it into the cache)\n\tvar module = __webpack_module_cache__[moduleId] = {\n\t\t// no module.id needed\n\t\t// no module.loaded needed\n\t\texports: {}\n\t};\n\n\t// Execute the module function\n\t__webpack_modules__[moduleId](module, module.exports, __webpack_require__);\n\n\t// Return the exports of the module\n\treturn module.exports;\n}\n\n","// define getter functions for harmony exports\n__webpack_require__.d = (exports, definition) => {\n\tfor(var key in definition) {\n\t\tif(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {\n\t\t\tObject.defineProperty(exports, key, { enumerable: true, get: definition[key] });\n\t\t}\n\t}\n};","__webpack_require__.o = (obj, prop) => (Object.prototype.hasOwnProperty.call(obj, prop))","// define __esModule on exports\n__webpack_require__.r = (exports) => {\n\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n\t}\n\tObject.defineProperty(exports, '__esModule', { value: true });\n};","// Content Script - Tab Audio Capture via Background Worker\n// File: src/content/content.ts\nimport { TranscriptRecognition } from './speechRecognition';\nconsole.log('Tab Audio Capture content script loaded!');\nlet recognition = null;\nlet transcriptOverlay = null;\n// Buffer management\nlet lastProcessedText = '';\nlet displayedTexts = new Set();\nlet transcriptBuffer = [];\nconst BUFFER_CLEANUP_INTERVAL = 30000;\n// Create enhanced overlay UI\nfunction createOverlay() {\n    if (transcriptOverlay)\n        return;\n    transcriptOverlay = document.createElement('div');\n    transcriptOverlay.id = 'transcript-overlay';\n    transcriptOverlay.style.cssText = `\n    position: fixed;\n    bottom: 20px;\n    left: 50%;\n    transform: translateX(-50%);\n    background: rgba(0, 0, 0, 0.92);\n    color: white;\n    padding: 20px 28px;\n    border-radius: 16px;\n    font-family: 'Segoe UI', system-ui, -apple-system, sans-serif;\n    font-size: 17px;\n    max-width: 85%;\n    max-height: 35vh;\n    overflow-y: auto;\n    z-index: 999999;\n    box-shadow: 0 8px 32px rgba(0, 0, 0, 0.5);\n    backdrop-filter: blur(10px);\n    display: none;\n    border: 1px solid rgba(255, 255, 255, 0.1);\n  `;\n    const headerDiv = document.createElement('div');\n    headerDiv.style.cssText = `\n    display: flex;\n    justify-content: space-between;\n    align-items: center;\n    margin-bottom: 12px;\n    padding-bottom: 8px;\n    border-bottom: 1px solid rgba(255, 255, 255, 0.1);\n  `;\n    const statusDiv = document.createElement('div');\n    statusDiv.id = 'transcript-status';\n    statusDiv.style.cssText = `\n    font-size: 12px;\n    color: #4CAF50;\n    display: flex;\n    align-items: center;\n    gap: 8px;\n    font-weight: 500;\n  `;\n    statusDiv.innerHTML = `\n    <span style=\"display: inline-block; width: 10px; height: 10px; background: #4CAF50; border-radius: 50%; animation: pulse 1.5s infinite;\"></span>\n    <span id=\"status-text\">Listening to Tab Audio...</span>\n  `;\n    const audioSourceDiv = document.createElement('div');\n    audioSourceDiv.id = 'audio-source';\n    audioSourceDiv.style.cssText = `\n    font-size: 11px;\n    color: #999;\n    display: flex;\n    align-items: center;\n    gap: 6px;\n  `;\n    audioSourceDiv.innerHTML = `\n    <span>üîä</span>\n    <span>Tab Audio (Direct)</span>\n  `;\n    headerDiv.appendChild(statusDiv);\n    headerDiv.appendChild(audioSourceDiv);\n    const textDiv = document.createElement('div');\n    textDiv.id = 'transcript-text';\n    textDiv.style.cssText = `\n    line-height: 1.7;\n    min-height: 32px;\n    color: rgba(255, 255, 255, 0.95);\n    letter-spacing: 0.3px;\n  `;\n    textDiv.innerHTML = `<span style=\"color: #999; font-style: italic;\">Waiting for audio...</span>`;\n    const style = document.createElement('style');\n    style.textContent = `\n    @keyframes pulse {\n      0%, 100% { opacity: 1; transform: scale(1); }\n      50% { opacity: 0.3; transform: scale(0.8); }\n    }\n    \n    @keyframes fadeIn {\n      from { opacity: 0; transform: translateY(10px); }\n      to { opacity: 1; transform: translateY(0); }\n    }\n    \n    #transcript-overlay::-webkit-scrollbar {\n      width: 6px;\n    }\n    \n    #transcript-overlay::-webkit-scrollbar-track {\n      background: rgba(255, 255, 255, 0.05);\n      border-radius: 3px;\n    }\n    \n    #transcript-overlay::-webkit-scrollbar-thumb {\n      background: rgba(255, 255, 255, 0.2);\n      border-radius: 3px;\n    }\n    \n    .transcript-segment {\n      animation: fadeIn 0.3s ease-out;\n      margin-bottom: 8px;\n    }\n    \n    .repaired-text {\n      background: rgba(76, 175, 80, 0.2);\n      padding: 2px 6px;\n      border-radius: 4px;\n      border-bottom: 2px solid #4CAF50;\n    }\n    \n    .low-confidence {\n      color: #FF9800;\n    }\n    \n    .medium-confidence {\n      color: #FFEB3B;\n    }\n  `;\n    document.head.appendChild(style);\n    transcriptOverlay.appendChild(headerDiv);\n    transcriptOverlay.appendChild(textDiv);\n    document.body.appendChild(transcriptOverlay);\n    console.log('Enhanced overlay created');\n    startBufferCleanup();\n}\nfunction startBufferCleanup() {\n    setInterval(() => {\n        const now = Date.now();\n        transcriptBuffer = transcriptBuffer.filter(item => now - item.timestamp < BUFFER_CLEANUP_INTERVAL);\n        if (displayedTexts.size > 50) {\n            displayedTexts.clear();\n        }\n    }, 10000);\n}\nfunction isTextAlreadyDisplayed(newText) {\n    const normalized = newText.toLowerCase().trim();\n    if (displayedTexts.has(normalized)) {\n        return true;\n    }\n    if (lastProcessedText) {\n        const lastNormalized = lastProcessedText.toLowerCase().trim();\n        if (lastNormalized.includes(normalized) && normalized.length > 5) {\n            return true;\n        }\n        const similarity = calculateSimilarity(normalized, lastNormalized);\n        if (similarity > 0.8) {\n            return true;\n        }\n    }\n    return false;\n}\nfunction calculateSimilarity(str1, str2) {\n    const longer = str1.length > str2.length ? str1 : str2;\n    const shorter = str1.length > str2.length ? str2 : str1;\n    if (longer.length === 0)\n        return 1.0;\n    const editDistance = getEditDistance(longer, shorter);\n    return (longer.length - editDistance) / longer.length;\n}\nfunction getEditDistance(str1, str2) {\n    const matrix = [];\n    for (let i = 0; i <= str2.length; i++) {\n        matrix[i] = [i];\n    }\n    for (let j = 0; j <= str1.length; j++) {\n        matrix[0][j] = j;\n    }\n    for (let i = 1; i <= str2.length; i++) {\n        for (let j = 1; j <= str1.length; j++) {\n            if (str2.charAt(i - 1) === str1.charAt(j - 1)) {\n                matrix[i][j] = matrix[i - 1][j - 1];\n            }\n            else {\n                matrix[i][j] = Math.min(matrix[i - 1][j - 1] + 1, matrix[i][j - 1] + 1, matrix[i - 1][j] + 1);\n            }\n        }\n    }\n    return matrix[str2.length][str1.length];\n}\nasync function repairTextWithAI(text, confidence) {\n    try {\n        const statusText = document.getElementById('status-text');\n        if (statusText) {\n            statusText.textContent = 'Processing with AI...';\n        }\n        const requestId = `${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n        const response = await Promise.race([\n            chrome.runtime.sendMessage({\n                action: 'repairText',\n                text: text,\n                confidence: confidence,\n                requestId: requestId\n            }),\n            new Promise((_, reject) => setTimeout(() => reject(new Error('Timeout')), 3000))\n        ]);\n        if (statusText) {\n            statusText.textContent = 'Listening to Tab Audio...';\n        }\n        if (response && response.success) {\n            return {\n                text: response.repairedText,\n                wasRepaired: response.wasRepaired\n            };\n        }\n        return { text, wasRepaired: false };\n    }\n    catch (error) {\n        console.error('AI Repair error:', error);\n        const statusText = document.getElementById('status-text');\n        if (statusText) {\n            statusText.textContent = 'Listening to Tab Audio...';\n        }\n        return { text, wasRepaired: false };\n    }\n}\nasync function updateTranscript(text, isFinal, confidence) {\n    if (!transcriptOverlay)\n        return;\n    if (!isFinal) {\n        return;\n    }\n    const textDiv = document.getElementById('transcript-text');\n    if (!textDiv)\n        return;\n    if (isTextAlreadyDisplayed(text)) {\n        console.log('‚è≠Ô∏è Skipping duplicate text:', text);\n        return;\n    }\n    const repairResult = await repairTextWithAI(text, confidence);\n    const repairedText = repairResult.text;\n    const wasRepaired = repairResult.wasRepaired;\n    let confidenceClass = '';\n    if (confidence < 0.5) {\n        confidenceClass = 'low-confidence';\n    }\n    else if (confidence < 0.7) {\n        confidenceClass = 'medium-confidence';\n    }\n    const repairClass = wasRepaired ? 'repaired-text' : '';\n    const segment = `<span class=\"transcript-segment ${confidenceClass} ${repairClass}\">${repairedText}</span> `;\n    if (textDiv.innerHTML.includes('Waiting for audio')) {\n        textDiv.innerHTML = '';\n    }\n    textDiv.innerHTML += segment;\n    lastProcessedText = repairedText;\n    displayedTexts.add(repairedText.toLowerCase().trim());\n    transcriptBuffer.push({\n        text: repairedText,\n        timestamp: Date.now()\n    });\n    textDiv.scrollTop = textDiv.scrollHeight;\n    const segments = textDiv.querySelectorAll('.transcript-segment');\n    if (segments.length > 10) {\n        for (let i = 0; i < segments.length - 10; i++) {\n            segments[i].remove();\n        }\n    }\n    console.log(`‚úÖ Displayed: \"${text}\" ‚Üí \"${repairedText}\" (confidence: ${confidence.toFixed(2)}, repaired: ${wasRepaired})`);\n}\nfunction showOverlay() {\n    if (transcriptOverlay) {\n        transcriptOverlay.style.display = 'block';\n    }\n}\nfunction hideOverlay() {\n    if (transcriptOverlay) {\n        transcriptOverlay.style.display = 'none';\n    }\n}\nfunction resetBuffer() {\n    lastProcessedText = '';\n    displayedTexts.clear();\n    transcriptBuffer = [];\n}\n// Listen for messages from popup\nchrome.runtime.onMessage.addListener((message, sender, sendResponse) => {\n    console.log('üì® Message received:', message);\n    if (message.action === 'ping') {\n        sendResponse({ success: true, message: 'Tab audio content script ready' });\n        return true;\n    }\n    if (message.action === 'start') {\n        (async () => {\n            try {\n                createOverlay();\n                resetBuffer();\n                // Initialize recognition - Speech Recognition API akan otomatis\n                // mendengar audio dari tab yang aktif (tidak perlu getDisplayMedia)\n                if (!recognition) {\n                    recognition = new TranscriptRecognition(async (text, isFinal, confidence) => {\n                        await updateTranscript(text, isFinal, confidence);\n                    });\n                }\n                recognition.start();\n                showOverlay();\n                sendResponse({ success: true, message: 'Started listening to tab audio' });\n            }\n            catch (error) {\n                console.error('‚ùå Start error:', error);\n                sendResponse({ success: false, message: String(error) });\n            }\n        })();\n        return true;\n    }\n    if (message.action === 'stop') {\n        if (recognition) {\n            recognition.stop();\n            hideOverlay();\n            resetBuffer();\n        }\n        sendResponse({ success: true, message: 'Stopped' });\n        return true;\n    }\n    return true;\n});\n"],"names":[],"sourceRoot":""}